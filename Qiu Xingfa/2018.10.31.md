# 2018.10.31

----------
[Analogical Reasoning on Chinese Morphological and Semantic Relations](http://aclweb.org/anthology/P18-2023)<br>
这篇论文来自ACL2018，主要是构造了一个更大更全面的中文词向量测试集CA8，之前的测试集CA_translated太小，而且很多都是直接从英文的测试集翻译过来的，不具有代表性，而CA8这个测试集则包含了形态和语义两个方面共17813条数据，然后作者用不同的语料不同方法训练出来的词向量进行测试<br>
open source:[Chinese Word Vectors 中文词向量](https://github.com/Embedding/Chinese-Word-Vectors)<br>
作者开源了上百种预训练中文词向量，我选了部分词向量，再加上之前腾讯公布的词向量，以及我自己训练的词向量进行了测试，论文里用的是他自己写的程序进行测试，我用的是gensim中的evaluate_word_analogies函数进行测试，计算方式略有不同，所以结果也略有不同<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/dfa85af086256cfc5be6617a85e0100.png)<br>
总体上测试结果基本与论文一致，增加bigram或character特征进行训练的词向量效果要比不增加的要好，其中增加bigram特征的词向量整体效果比不加特征略微要好一点，而增加character特征的词向量在形态方面的测试效果增加明显，语义方面效果则变化不大，而同时增加ngram和character特征的词向量基本上能够兼顾两者的优点<br>
所有语料的集合merge效果比其他单独的语料要好，也基本上比腾讯的词向量效果要好，
<br>
<br>

----------
[Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings](http://www.cips-cl.org/static/anthology/CCL-2018/CCL-18-086.pdf)<br>
这篇论文探讨了词向量的intrinsic evaluation和extrinsic evaluation的关系，使用character和ngram特征的效果基本上比不使用的要好，其中character特征在intrinsic task中效果比较明显，而ngram特征在extrinsic中效果比较明显，基本上，语料越大效果越好，训练语料和测试数据属于同一领域时效果要好


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>






---------
## [Analogical Reasoning on Chinese Morphological and Semantic Relations](http://aclweb.org/anthology/P18-2023)
### Morphological Relations
* **Reduplication**<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/6295e828eb06283de9d5e4897935db9.png)<br>

* **Semi-affixation**
    * 一 → 第一
        二 → 第二
    * 胖 → 胖子
        瘦 → 瘦子

### Semantic Relations
* geography,history, nature, and people<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/f51e8a2f7c1f84477447a05102281da.png)<br>

### Experiments
* **Vector Representations**
    * SGNS (skip-gram model with negative sampling)
    * PPMI (Positive Pointwise Mutual Information)<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/34666fedd4ab4bb4b4f20fe2c45e121.png)<br>
    * the training procedure of SGNS favors frequent word pairs
    * PPMI model is more sensitive to infrequent and specific word pairs, which are beneficial to semantic relations
    * Table 4 shows that there is only a `slight increase` on `CA_translated` dataset with ngram features, and the accuracies in most cases `decrease` after integrating character features. 
    * In contrast, on `CA8 dataset`, the introduction of ngram and character features brings `significant and consistent improvements` on almost all the categories.
* **Corpora**<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/bdadf99aee7aa1c65a828385fd52981.png)<br>
    * accuracies increase with the growth in corpus size
    * vectors trained on news data are beneficial to geography relations, especially on People’s Daily which has a focus on political news

--------------
## [Revisiting Correlations between Intrinsic and Extrinsic Evaluations of Word Embeddings](http://www.cips-cl.org/static/anthology/CCL-2018/CCL-18-086.pdf)

### Related Work
* the effectiveness of `word similarity task` in evaluation has been `questioned` a lot, for example, human judgment of word similarity is subjective and similarity is often confused with relatedness

### Intrinsic Tasks
* **Word Similarity**
* **Word Analogy**

### Extrinsic Tasks
* **Named Entity Recognition**
    * use a hybrid BiLSTM-CRF model
* **Sentiment Classification**
    * train a simple but effective CNN model for binary sentiment classification

### Experiments

We train word embeddings with SGNS (Skip-gram with negative-Sampling) model implemented by `ngram2vec` toolkit

* **Results and Analysis of NER**
    * Context features
    * Corpus size
    * Corpus domain
* **Results and Analysis of Sentiment Classification**<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/409de10fdbbf504c806da9a85ef9f23.png)<br>
* **Results of Intrinsic Evaluation**<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/45e4a6e804dfe8d9363d119de830750.png)<br>
* **Correlation between Intrinsic Evaluation and Extrinsic Evaluation**
    * By introducing the character and ngram features, performances of both intrinsic and extrinsic tasks `improve`, but we can observe that `character features` are more favorable to `intrinsic` tasks, while `ngram features` prove to be more advantageous for `extrinsic tasks`
    * By comparing embeddings trained with corpora of different sizes and domains, we can find that larger size or similar domain can be important advantages for `both` intrinsic and extrinsic tasks. And the `combination corpus` with largest size and varied domains `always performs the best`.<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/75a4ffde55f9f6871ff06ab1fed1d01.png)<br>
    * It is not surprising that the correlation between morphological reasoning and semantic reasoning is high
            

