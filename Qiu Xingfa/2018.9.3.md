# 2018.9.3


## Bidirectional LSTM-CRF Models for Sequence Tagging

1. LSTM Networks<br>
* A RNN maintains a memory based on `history information`, which enables the model to predict the current output conditioned on `long distance` features.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/10dc9e2c71da3ef8a92df7eb386e22b.png)

2. Bidirectional LSTM Networks<br>
* we can efficiently make use of `past` features (via forward states) and `future` features (via backward states) for a specific time frame<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/c103f4a9381b0fe187ebb2fac555311.png)

3. CRF networks<br>
* CRFs can produce higher `tagging` accuracy in general.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/65844e99cf5c61ace0e84c4e81b1255.png)

4. LSTM-CRF networks<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/c24d0638b34b16c4195c37e3a142eee.png)

5. BI-LSTM-CRF networks<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/ee00e62d922ebf34012d3ada05794f0.png)

6. data<br>
Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/52ed854f41e5c81d84537b46cf45175.png)

7. Features<br>
* Spelling features
* Context features
* Word embedding
* Features connection tricks

8. Results<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.5/31539f775b02e85c67b2848f0c6c24b.png)

## 下周计划
想办法自己运行一下这几个模型并进行比较<br>
学习斯坦福NLP课程









