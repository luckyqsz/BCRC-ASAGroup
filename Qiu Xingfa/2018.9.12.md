# 2018.9.12
---
## word2vec
来产生词向量的相关模型<br>
[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf)<br>
[Word2Vec Tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)<br>
[Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)<br>

1. One-hot Representation<br>
* “可爱”表示为 [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 ...] 
* “面包”表示为 [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 ...]
* 向量的维度会随着句子的词的数量类型增大而增大
* 任意两个词之间都是孤立的，无法表示语义层面上词汇之间的相关信息，而这一点是致命的。
2. 词的分布式表示
* `Harris` 在1954年提出的“分布假说”为这一设想提供了理论基础：上下文相似的词，其语义也相似。`Firth` 在1957年对分布假说进行了进一步阐述和明确：词的语义由其上下文决定。
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131555uwstwas4a6tkwzl8.jpg)
3. 词嵌入
* 基于神经网络的分布表示一般称为`词向量`、`词嵌入`（ word embedding）或`分布式表示`（distributed representation）。核心依然是上下文的表示以及上下文与目标词之间的关系的建模。
* 将vector每一个元素由整形改为浮点型，变为整个实数范围的表示；将原来稀疏的巨大维度压缩嵌入到一个更小维度的空间
* 词向量是训练神经网络时候的隐藏层参数或者说矩阵。
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131556ohn99d9onh2np93k.jpg)<br>
![](http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png)
4. Skip-gram (Continuous Skip-gram Model)<br>
当前词预测上下文<br>
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131556rqqoke9o4ujzufjq.jpg)<br>
CBOW (Continuous Bag-of-Words Model)
上下文来预测当前词
![](http://attachbak.dataguru.cn/attachments/portal/201805/03/131556wjk9opzrktwpdttj.jpg)

5. 优化方法<br>
* Treating `common word pairs` or `phrases` as single “words” in their model.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/75e019f1b267bb4472b14646f2be512.png)
* `Subsampling frequent words` to decrease the number of training examples.<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/bfde864b66e81146ebc712b0a5e573a.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/17e7ae7a9e05e4888c5db79d01f0557.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/12a6ddabfab42c4ed8fb2b23482fe6b.png)<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/48fa21c69323c6dde966ce5e4c8bf19.png)<br>
* Negative Sample<br>
把语料中的一个词串的中心词替换为别的词，构造语料中不存在的词串作为`负样本`。在这种策略下，优化目标变为了：较大化正样本的概率，同时最小化负样本的概率。<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.9.12/1f417de699cc55846cf6cf80cf78660.png)<br>
>which causes each training sample to update only a small percentage of the model’s weights<br><br>
Recall that the output layer of our model has a weight matrix that's `300 x 10,000`. So we will just be updating the weights for our positive word (“quick”), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and `1,800` weight values total. That’s only 0.06% of the 3M weights in the output layer!<br>

* Hierarchical Softmax(还在理解中）
>Hierarchical Softmax是一种对输出层进行优化的策略，输出层从原始模型的利用softmax计算概率值改为了利用Huffman树计算概率值。一开始我们可以用以词表中的全部词作为叶子节点，词频作为节点的权，构建Huffman树，作为输出。从根节点出发，到达指叶子节点的路径是的。Hierarchical Softmax正是利用这条路径来计算指定词的概率，而非用softmax来计算。

## 下周计划

* 实体关系抽取调研<br>
[Must-read papers on NRE](https://github.com/thunlp/NREPapers)
* 学习斯坦福NLP课程
        
        




