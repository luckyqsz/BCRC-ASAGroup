# 2018.10.24
-------
读了两篇论文：<br>
[Directional Skip-Gram: Explicitly Distinguishing Left and Right Context for Word Embeddings](http://aclweb.org/anthology/N18-2028)<br>
主要是在原来的skip-gram模型基础上增加一个方向向量，表示该词是在中心词的左边还是右边，得到的效果比原来的skip-gram模型要好，模型的复杂程度也要比带有单词顺序特征的Structured Skip-Gram Model等模型要低<br>
<br>
[A Survey of Deep Learning Methods for Relation Extraction](https://arxiv.org/pdf/1705.03645.pdf)<br>
关于在关系抽取方面使用深度学习方法的总结，主要是`Supervised learning with CNNs`使用了CNN的有监督学习，`Multi-instance learning models with distant supervision`远程监督的多示例学习<br>
<br>
跑了两份代码：<br>
[命名实体识别](https://github.com/Determined22/zh-NER-TF)<br>
[从中文文本中自动提取关键词和摘要](https://github.com/letiantian/TextRank4ZH)<br>

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

















--------------------------
## Directional Skip-Gram: Explicitly Distinguishing Left and Right Context forWord Embeddings

* Structured Skip-Gram Model (**SSG**)is an adaptation of SG model with consideration of words'order
* Directional skip-gram (**DSG**), with consideration of not only the word co-occurrence patterns, but also their relative positions modeled by a special “direction” vector, which indicates whether the word to be predicted is on the left or right side of the given word.
* f(wt+i,wt) = p(wt+i |wt) + g(wt+i,wt).
* **Complexity Analysis**<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/9cf5647062d6b7c05ead04c8b32911f.png)<br>
* Simplified SSG (**SSSG**) model that only models left and right context for a given word
* **Dataset**: [Wikipedia articles](https://dumps.wikimedia.org/enwiki/latest/)<br>
    **Comparison**: SG, SSG, SSSG<br>
    **Settings**: <br>
        ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/41600009d743a0e53f7a1c24b57a703.png)<br>
* **Training Speed**<br>
        ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/95df93d000745ca5d8e8435d52d14a8.png)<br>
* **Word Similarity Evaluation**<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/b39f186f231aee55fd4fb99177370a6.png)<br>
    ![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/3da08ca55027667481e7a5c455e8133.png)<br>
    The reason may be largely due to that modeling order dependence is sensitive to `data sparsity`, hence CWin model fails to generate meaningful representations for low-frequency words,which are prevalent on small corpus.
    DSG yields robust results on different scale of training data, which suggests that our model provides an effective solution to learn embeddings with exploiting the structure in context, while not severely suffered from the data sparsity problem
* **Part-of-Speech Tagging**
    POS prediction is conducted by a bidirectional LSTM-CRF<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/9fd49714ad0c1149cc9e41bd58ccd22.png)<br>

----------
## A Survey of Deep Learning Methods for Relation Extraction

### Introduction
* Information Extraction itself is a huge task consisting of several sub tasks like `named-entity-recognition`, `relation extraction`, `event extraction` etc.
* Traditional non deep learning methods
    * feature based methods
    * kernel based methods
* Mintz proposed a `distant supervision` method for producing large amount of training data by aligning KB facts with texts.

### Datasets
* ACE 2005 dataset
* SemEval-2010 Task 8 dataset
* **Distant Supervision**
    * the `assumption` that if a relation exists between an entity pair in the KB, then every document containing the mention of the entity pair would express that relation.
    * Riedel relaxed the distant supervision assumption by modeling the problem as a multiinstance learning problem.
* Positional Embeddings
    * words closer to the target entities usually contain more useful information regarding the relation class.

### Supervised learning with CNNs
* Simple CNN model
    * The model uses `synonym vectors` instead of word vectors.
* CNN model with max-pooling
    * they use word embeddings and `Positional Embeddings`.they use  a max-pooling layer over the output of the convolutional network.
* CNN with multi-sized window kernels
    * they also incorporate convolutional kernels of `varying window sizes` to capture wider ranges of n-gram features.

### Multi-instance learning models with distant supervision
* It is a form of supervised learning where a label is given to a bag of instances. The bag consists of all the sentences that contain the mention of one entity pair.
* Piecewise Convolutional Neural Networks
    * piecewise max-pooling across the sentence.
    * max-pooling in different segments of the sentence instead of the entire sentence.
    * it uses only the one most-likely document for the entity pair during the training and prediction stage.using only a single sentence is a very hard constraint.<br>
    * <br>![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/a96130e204e58183615f633ca5b59d9.png)<br>
* Selective Attention over Instances
    * used an attention mechanism
* Multi-instance Multi-label CNNs
    * the same entity pair can have multiply relations
* <br>![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/4d043f2dabc6bd8823468a6742073e6.png)<br>
* the deep learning models perform significantly better than the non deep  learning models









