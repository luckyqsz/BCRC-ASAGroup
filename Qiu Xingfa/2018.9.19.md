# 2018.9.19

---

1. 看了TensorFlow中的[word2vec_basic](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)的代码，初步理解word2vec原理，运行了一下代码<br>
2. 参数
data : text8<br>
vocabulary_size = 50000<br>
batch_size = 128<br>
embedding_size = 128<br>
skip_window = 1<br>
num_skips = 2 `每个词的上下文中随机选择的训练实例数`<br>
num_sampled = 64 `训练时用来做负样本的噪声单词的数量`<br>
num_steps = 100001<br>
3. 结果
展示词频最高的500个单词的可视化结果<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/tsne.png)<br>
4. 使用搜狗实验室部分[搜狐新闻数据](http://www.sogou.com/labs/resource/cs.php)<br>
提取中文并进行分词，得到<br>
Vocab size: 81115<br>
Words in train file: 8202162<br>
结果：<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/0a660c1140f2fe56887266aaf17b1b2.png)<br>
选取了五个词进行了可视化<br>
上海、中央、学生、快乐、商品<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/%E6%96%B0%E9%97%BB.png)<br>
5. 在微博爬取了[@人民网](https://weibo.com/renminwang)和[@人民日报](https://weibo.com/rmrb)共153984条微博,提取中文并进行分词，得到<br>
Vocab size: 76470<br>
Words in train file: 8208476<br>
结果：<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/7b6eb1e790000e05ac701c3f9b8f3f6.png)<br>
同样选取了五个词进行了可视化<br>
上海、中央、学生、快乐、商品<br>
![](https://github.com/qiuxingfa/picture_/blob/master/2018.10/%E4%BA%BA%E6%B0%91.png)
6. 通过结果的对比，可以看出搜狐新闻的数据量和微博的数据量虽然很接近，但微博数据的效果明显要差一些，这可能是因为微博文本更加短小和灵活，语言表达没有那么规范。
7. 用上下文来确定词向量的做法似乎对多义词不太友好<br>
解决办法之一：主题向量






